{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac96f5-b3f3-470c-ba50-71a78ed1124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Bagging (Bootstrap Aggregating) reduces overfitting in decision trees by reducing the variance\n",
    "of the model. It does this by training multiple decision tree models on different bootstrap samples\n",
    "of the data and averaging their predictions. By combining multiple models, bagging reduces the impact\n",
    "of outliers and noise in the data, which can lead to overfitting in a single model.\n",
    "\n",
    "Q2. The advantage of using different types of base learners in bagging is that it can improve the \n",
    "diversity of the ensemble, leading to better performance. For example, using decision trees and neural \n",
    "networks as base learners can capture different aspects of the data and improve the overall predictive\n",
    "performance. The disadvantage is that it can increase the computational complexity and \n",
    "training time of the ensemble.\n",
    "\n",
    "Q3. The choice of base learner can affect the bias-variance tradeoff in bagging. \n",
    "A low-bias base learner, such as a decision tree with high depth, can lead to a high-variance ensemble.\n",
    "On the other hand, a high-bias base learner, such as a decision tree with low depth, can lead to a\n",
    "low-variance ensemble with higher bias. Therefore, the choice of base learner should be based on the \n",
    "tradeoff between bias and variance that best suits the problem.\n",
    "\n",
    "Q4. Bagging can be used for both classification and regression tasks. In classification tasks,\n",
    "bagging is often used with decision tree models to improve the accuracy of the classification.\n",
    "In regression tasks, bagging is used to improve the stability of the regression model and to reduce\n",
    "the variance of the predictions. The main difference between the two cases is the loss function used\n",
    "in training the base learners.\n",
    "\n",
    "Q5. The ensemble size in bagging refers to the number of models included in the ensemble.\n",
    "The role of ensemble size is to balance the variance and bias of the ensemble. As the ensemble \n",
    "size increases, the variance of the ensemble decreases, but the bias increases. \n",
    "The optimal ensemble size depends on the complexity of the problem and the choice of base learner. \n",
    "In practice, it is often found that an ensemble size between 50 and 200 models provides good performance.\n",
    "\n",
    "Q6. One real-world application of bagging in machine learning is in predicting the risk of credit default.\n",
    "A bagged ensemble of decision trees can be trained on historical credit data to predict the risk\n",
    "of default for new credit applications. The ensemble can incorporate multiple features, \n",
    "such as credit history, income, and demographic data, to provide a more accurate prediction of \n",
    "the risk of default. By using bagging, the ensemble can reduce the impact of outliers and noise\n",
    "in the data, leading to more stable and accurate predictions.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
