{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f891b6d-62e4-4809-86ee-e641de331904",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. The mathematical formula for a linear SVM is f(x) = (w dot x + b).\n",
    "\n",
    "Q2. The objective function of a linear SVM is to maximize the margin, which can be written as:\n",
    "minimize (1/2) ||w||^2 subject to y_i(w dot x_i + b) >= 1 for all i = 1, ..., n\n",
    "\n",
    "Here, ||w|| is the Euclidean norm of the weight vector, y_i is the label of the ith training example,\n",
    "and x_i is the corresponding feature vector.\n",
    "\n",
    "Q3. The kernel trick in SVM is used to implicitly map the input data into a higher-dimensional \n",
    "feature space without actually computing the coordinates of the data in that space. \n",
    "This is done by replacing the dot product w dot x between the weight vector and the input vector\n",
    "with a kernel function K(x, x') that computes the similarity between the two vectors:\n",
    "f(x) = sign(sigma_i alpha_i y_i K(x, x_i) + b)\n",
    "Here, alpha_i are the coefficients of the support vectors, which are the training examples that lie\n",
    "closest to the decision boundary.\n",
    "\n",
    "Q4. The role of support vectors in SVM is to define the decision boundary and determine the margin.\n",
    "Only the support vectors have non-zero coefficients alpha_i, and they lie on or close to the margin. \n",
    "An example of support vectors in a binary classification problem is shown in the following formula:\n",
    "\n",
    "f(x) = sign(sigma_i alpha_i y_i K(x, x_i) + b)\n",
    "\n",
    "Here, the support vectors are the data points with non-zero alpha_i values, and they lie on\n",
    "the margin or on the wrong side of the margin.\n",
    "\n",
    "Q5. The following are the formulas for the different types of SVM margins:\n",
    "\n",
    "Hyperplane: The hyperplane is the decision boundary of the SVM and can be expressed as w dot x + b = 0.\n",
    "Marginal plane: The marginal plane is parallel to the hyperplane and is located at a distance of 1/||w||\n",
    "from the hyperplane on both sides.\n",
    "Soft margin: The soft margin allows for some misclassifications by introducing slack variables eta_i\n",
    "for each training example, which measure the degree of misclassification.\n",
    "The objective function for the soft margin SVM can be written as:\n",
    "minimize (1/2) ||w||^2 + C sigma_i eta_i subject to y_i(w dot x_i + b) >= 1 - eta_i \n",
    "for all i = 1, ..., n and Î¾_i >= 0 for all i = 1, ..., n\n",
    "\n",
    "Here, C is a hyperparameter that controls the tradeoff between maximizing the margin and minimizing \n",
    "the misclassifications.\n",
    "\n",
    "Hard margin: The hard margin SVM requires that all training examples are correctly classified,\n",
    "and there are no slack variables. The objective function for the hard margin SVM can be written as:\n",
    "minimize (1/2) ||w||^2 subject to y_i(w dot x_i + b) >= 1 for all i = 1, ..., n\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09c6d90a-38ad-442e-93d4-926a8e2995d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76dbfe50-648e-4877-90aa-846e8ab5f81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11557c15-1ddd-47a5-88ae-659e20cde94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "C = 0.1, Accuracy = 1.0\n",
      "C = 1, Accuracy = 1.0\n",
      "C = 10, Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier on the training set\n",
    "svm = LinearSVC()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model on the testing set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Try different values of the regularisation parameter C and see how it affects the performance of the model\n",
    "for C in [0.1, 1, 10]:\n",
    "    svm = LinearSVC(C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"C = {C}, Accuracy = {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9550e-57a1-45e9-b9fd-fb4af874abea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
