{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce266dd7-6f5c-40d1-944b-37e736577d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Linear regression is a type of regression analysis used to model the relationship between a \n",
    "dependent variable and one or more independent variables. The output of linear regression is a\n",
    "continuous numerical value, and the model is fitted to minimize the sum of squared errors.\n",
    "In contrast, logistic regression is used to model the probability of a binary outcome based \n",
    "on one or more independent variables. The output of logistic regression is a probability score \n",
    "between 0 and 1, and the model is fitted to maximize the log-likelihood function.\n",
    "For example, if we want to predict the price of a house based on its square footage, the number\n",
    "of bedrooms, and the location, linear regression would be appropriate. However, if we want to \n",
    "predict whether a customer will purchase a product based on their demographic information and\n",
    "browsing history, logistic regression would be more appropriate.\n",
    "\n",
    "Q2. The cost function used in logistic regression is the negative log-likelihood function,\n",
    "which measures the discrepancy between the predicted probabilities and the actual outcomes.\n",
    "The goal is to minimize this function to obtain the optimal parameters of the model.\n",
    "The optimization is typically done using gradient descent, where the gradient of the cost function \n",
    "with respect to each parameter is computed and used to update the parameter values iteratively.\n",
    "\n",
    "Q3. Regularization is a technique used to prevent overfitting in logistic regression by adding a \n",
    "penalty term to the cost function that discourages large parameter values. The two common types \n",
    "of regularization are L1 regularization (lasso regression) and L2 regularization (ridge regression). \n",
    "L1 regularization encourages sparsity in the model by setting some of the parameter values to zero,\n",
    "while L2 regularization shrinks the parameter values towards zero. Regularization helps to prevent\n",
    "overfitting by reducing the complexity of the model and improving its generalizability.\n",
    "\n",
    "Q4. The ROC (Receiver Operating Characteristic) curve is used to evaluate the performance of the \n",
    "logistic regression model by plotting the true positive rate (sensitivity) against the false positive \n",
    "rate (1-specificity) at different probability thresholds. The area under the curve (AUC) is a \n",
    "measure of the model's ability to discriminate between positive and negative examples. A model \n",
    "with an AUC of 0.5 is no better than random guessing, while a model with an AUC of 1.0 is perfect.\n",
    "\n",
    "Q5. Common techniques for feature selection in logistic regression include backward elimination, \n",
    "forward selection, and stepwise selection. Backward elimination starts with all the variables in \n",
    "the model and removes the variables one by one based on their significance level until only the \n",
    "significant variables are left. Forward selection starts with an empty model and adds the \n",
    "variables one by one based on their significance level until no more significant variables\n",
    "can be added. Stepwise selection combines forward and backward selection and includes a stopping \n",
    "rule to avoid overfitting. These techniques help to identify the most important variables and \n",
    "remove the irrelevant ones, thereby improving the model's performance.\n",
    "\n",
    "Q6. Imbalanced datasets in logistic regression refer to datasets where the number of examples\n",
    "in one class is much higher or lower than the other class. This can lead to biased models that \n",
    "are more accurate for the majority class and less accurate for the minority class. Strategies \n",
    "for dealing with class imbalance include resampling techniques such as oversampling the minority\n",
    "class, undersampling the majority class, and generating synthetic examples using techniques such\n",
    "as SMOTE (Synthetic Minority Over-sampling Technique). Another approach is to use cost-sensitive\n",
    "learning, where the misclassification cost is different for each class, and the model is \n",
    "optimized to minimize the total cost.\n",
    "\n",
    "Q7. Common issues and challenges when implementing logistic regression include multicollinearity\n",
    "among the independent variables, outliers, missing data, and non-linearity. Multicollinearity \n",
    "occurs when two or more independent variables are highly correlated, which can lead to unstable\n",
    "and unreliable estimates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
