{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fcd584-c907-4474-a61b-76d7748fc94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q1. Lasso Regression is a regularization technique used in linear regression that helps to\n",
    "prevent overfitting by shrinking the coefficients of less important features to zero, \n",
    "effectively performing feature selection and simplifying the model. It differs from other\n",
    "regression techniques, such as ordinary linear regression, by including a penalty term that \n",
    "is proportional to the absolute value of the coefficients\n",
    "\n",
    "Q2. The main advantage of using Lasso Regression in feature selection is that it can automatically\n",
    "select the most relevant features and discard the less important ones, leading to a simpler and \n",
    "more interpretable model. This can also help to reduce the risk of overfitting, especially when \n",
    "dealing with high-dimensional data with many input features\n",
    "\n",
    "Q3. The coefficients of a Lasso Regression model can be interpreted as the contribution of each \n",
    "input feature to the target variable, after taking into account the effect of all other features\n",
    "in the model. A positive coefficient means that the feature has a positive effect on the target \n",
    "variable, while a negative coefficient means the opposite\n",
    "\n",
    "Q4. The main tuning parameter in Lasso Regression is the regularization parameter, also known as \n",
    "lambda, which controls the strength of the penalty term. A higher value of lambda will result in \n",
    "a more severe penalty on the coefficients, leading to a simpler and more regularized model. \n",
    "However, if lambda is set too high, the model may underfit the data and result in poor performance. \n",
    "Another parameter that can be adjusted is the intercept term, which can be included or excluded in the model\n",
    "\n",
    "\n",
    "Q5. Yes, Lasso Regression can be used for non-linear regression problems by transforming the \n",
    "input features into non-linear functions before fitting the model.\n",
    "This can be done using techniques such as polynomial regression, spline regression, or kernel\n",
    "regression, which allow the model to capture non-linear relationships between the input and \n",
    "output variables\n",
    "\n",
    "Q6. The main difference between Ridge Regression and Lasso Regression is the type of penalty\n",
    "term used to regularize the coefficients. Ridge Regression uses the L2 penalty, \n",
    "which shrinks the coefficients towards zero by adding the squared sum of the coefficients \n",
    "to the cost function. Lasso Regression uses the L1 penalty, which shrinks the coefficients\n",
    "towards zero by adding the absolute sum of the coefficients to the cost function. \n",
    "As a result, Lasso Regression tends to produce more sparse models with fewer non-zero coefficients\n",
    "than Ridge Regression\n",
    "\n",
    "Q7. Yes, Lasso Regression can handle multicollinearity in the input features by shrinking the \n",
    "coefficients of highly correlated features towards zero, effectively performing feature selection \n",
    "and reducing the risk of overfitting. However, it is important to note that Lasso Regression may\n",
    "not always select the same subset of features as other regularization techniques, such as Ridge \n",
    "Regression, when dealing with multicollinearity.\n",
    "\n",
    "Q8. The optimal value of the regularization parameter in Lasso Regression can be chosen using \n",
    "techniques such as cross-validation, where the dataset is split into training and validation sets,\n",
    "and the model is trained and evaluated using different values of lambda. \n",
    "The value of lambda that produces the best performance on the validation set is then chosen \n",
    "as the optimal value. Another approach is to use information criteria, such as the Akaike\n",
    "Information Criterion (AIC) or the Bayesian Information Criterion (BIC), \n",
    "which balance the goodness of fit with the complexity of the model\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
