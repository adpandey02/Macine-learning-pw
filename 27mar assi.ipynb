{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a3127-b197-4dcd-941b-330069be3dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. The concept of R-squared in linear regression models represents the goodness of\n",
    "fit of the model to the data. It is a statistical measure that indicates how well the \n",
    "regression line fits the observed data. R-squared is calculated by dividing the explained \n",
    "variation by the total variation. The formula for R-squared is:\n",
    "R-squared = 1 - (SSres / SStot)\n",
    "where SSres is the sum of squares of residuals, and SStot is the total sum of squares.\n",
    "R-squared ranges from 0 to 1, where 1 indicates that the regression line perfectly fits \n",
    "the data and 0 indicates no correlation between the dependent and independent variables.\n",
    "\n",
    "Q2. Adjusted R-squared is a modification of R-squared that adjusts for the number of independent \n",
    "variables in the model. It is a more accurate measure of the goodness of fit of a regression model,\n",
    "especially when there are multiple independent variables. Adjusted R-squared penalizes the\n",
    "addition of irrelevant independent variables and rewards the addition of relevant variables.\n",
    "The formula for adjusted R-squared is:\n",
    "Adjusted R-squared = 1 - [(1-RÂ²)(n-1)/(n-p-1)]\n",
    "where n is the sample size, and p is the number of independent variables in the model.\n",
    "\n",
    "Q3. Adjusted R-squared is more appropriate to use when there are multiple independent variables \n",
    "in the model. It is a better measure of the goodness of fit of the model as it accounts for \n",
    "the number of independent variables in the model.\n",
    "\n",
    "Q4. RMSE, MSE, and MAE are evaluation metrics used in regression analysis to measure the performance\n",
    "of a regression model. RMSE stands for Root Mean Squared Error, MSE stands for Mean Squared Error, \n",
    "and MAE stands for Mean Absolute Error\n",
    "RMSE is the square root of the average of squared differences between the predicted and actual values.\n",
    "The formula for RMSE is:\n",
    "RMSE = sqrt(sum((y_pred - y_actual)^2)/n)\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the sample size.\n",
    "MSE is the average of squared differences between the predicted and actual values. The formula for MSE is:\n",
    "MSE = sum((y_pred - y_actual)^2)/n\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the sample size.\n",
    "MAE is the average of absolute differences between the predicted and actual values. The formula for MAE is:\n",
    "MAE = sum(abs(y_pred - y_actual))/n\n",
    "where y_pred is the predicted value, y_actual is the actual value, and n is the sample size.\n",
    "\n",
    "Q5. The advantage of using RMSE, MSE, and MAE as evaluation metrics is that they provide a quantitative \n",
    "measure of the performance of a regression model. They are easy to understand and interpret.\n",
    "However, these metrics have some disadvantages as well. For example, they are sensitive to \n",
    "outliers and can be affected by the scale of the data. RMSE is sensitive to large errors and\n",
    "MSE gives more weight to larger errors. MAE is less sensitive to outliers, but it does not \n",
    "differentiate between overestimation and underestimation of errors.\n",
    "\n",
    "Q6. Lasso regularization is a technique used in linear regression to prevent overfitting by \n",
    "adding a penalty term to the loss function. The penalty term is the absolute value of the \n",
    "coefficients, which results in sparse models. Lasso regularization shrinks the coefficients \n",
    "of the independent variables towards zero and eliminates irrelevant variables from the model.\n",
    "It is more appropriate to use when there are many independent variables, and some of them are\n",
    "not relevant to the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805089e5-da14-4244-bb1d-70d4d72ebfc6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nQ7. Regularized linear models help prevent overfitting in machine learning by adding a penalty \\nterm to the loss function that controls the complexity of the model.\\nThe penalty term discourages the model from assigning large weights to the features and forces it\\nto choose only the most relevant features. Regularization can be achieved by using \\nL1 regularization (Lasso) or L2 regularization (Ridge).\\n\\nFor example, consider a linear regression problem with 10 independent variables.\\nThe data set contains 1000 observations, and the model is trained using the ordinary least squares\\n(OLS) method. The OLS method may lead to overfitting if the model is too complex or if\\nthere are many irrelevant variables in the data set. To prevent overfitting,\\nwe can use Lasso or Ridge regression.\\n\\nSuppose we apply Lasso regression to the same data set.\\nLasso adds a penalty term to the loss function that is proportional to the absolute value of\\nthe coefficients. This penalty term forces the model to shrink the coefficients towards zero,\\nwhich eliminates the irrelevant variables and selects only the most relevant variables.\\nAs a result, the Lasso model will have fewer variables than the OLS model, \\nwhich reduces the risk of overfitting.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Q7. Regularized linear models help prevent overfitting in machine learning by adding a penalty \n",
    "term to the loss function that controls the complexity of the model.\n",
    "The penalty term discourages the model from assigning large weights to the features and forces it\n",
    "to choose only the most relevant features. Regularization can be achieved by using \n",
    "L1 regularization (Lasso) or L2 regularization (Ridge).\n",
    "\n",
    "For example, consider a linear regression problem with 10 independent variables.\n",
    "The data set contains 1000 observations, and the model is trained using the ordinary least squares\n",
    "(OLS) method. The OLS method may lead to overfitting if the model is too complex or if\n",
    "there are many irrelevant variables in the data set. To prevent overfitting,\n",
    "we can use Lasso or Ridge regression.\n",
    "\n",
    "Suppose we apply Lasso regression to the same data set.\n",
    "Lasso adds a penalty term to the loss function that is proportional to the absolute value of\n",
    "the coefficients. This penalty term forces the model to shrink the coefficients towards zero,\n",
    "which eliminates the irrelevant variables and selects only the most relevant variables.\n",
    "As a result, the Lasso model will have fewer variables than the OLS model, \n",
    "which reduces the risk of overfitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd6db07-e497-4701-8741-fd3b3f397311",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Q8. Limited applicability: Regularized linear models assume that the relationship between the \n",
    "independent variables and the dependent variable is linear. In cases where the relationship is \n",
    "highly nonlinear, these models may not be the best choice.\n",
    "Biased results: Regularized linear models can introduce bias into the model by shrinking \n",
    "the coefficients towards zero. This can be a problem if there are variables that are truly\n",
    "important predictors of the dependent variable but are not selected by the regularization process.\n",
    "Overfitting to noise: Regularized linear models can still overfit to noise in the data if the \n",
    "regularization strength is not set correctly or if the model is trained on a dataset that is too small\n",
    "\n",
    "Q9. In this case, Model B has a lower MAE, which means that its average absolute error is smaller \n",
    "than Model A. This could suggest that Model B is better at predicting the dependent variable \n",
    "than Model A, at least in terms of the average error. However, Model A has a higher RMSE,\n",
    "which means that its errors tend to be larger than Model B's errors on average.\n",
    "This could suggest that Model A is worse than Model B at predicting the dependent variable, \n",
    "especially for larger errors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38cb17fc-2c8a-49b8-91ce-4e191925da06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Lasso: 4.71\n",
      "MSE for Ridge: 0.28\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Choosing the better performing regularized linear model depends on the specific context and \n",
    "requirements of the problem at hand. However, in general, Ridge regularization tends to be more \n",
    "effective when dealing with highly correlated features, while Lasso regularization tends to be \n",
    "more effective when dealing with sparse datasets and features with low importance.\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some synthetic data\n",
    "np.random.seed(123)\n",
    "X = np.random.rand(1000, 10)\n",
    "y = X.dot(np.array([1, 2, 3, 4, 5, 0, 0, 0, 0, 0])) + np.random.normal(0, 0.5, 1000)\n",
    "\n",
    "# Train the models\n",
    "lasso = Lasso(alpha=0.5)\n",
    "lasso.fit(X, y)\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "y_pred_lasso = lasso.predict(X)\n",
    "mse_lasso = mean_squared_error(y, y_pred_lasso)\n",
    "y_pred_ridge = ridge.predict(X)\n",
    "mse_ridge = mean_squared_error(y, y_pred_ridge)\n",
    "\n",
    "print(\"MSE for Lasso: {:.2f}\".format(mse_lasso))\n",
    "print(\"MSE for Ridge: {:.2f}\".format(mse_ridge))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85a89d-8480-4b32-b1de-0118c0f717eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
