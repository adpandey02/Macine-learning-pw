{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd36cdd9-4603-4d8c-aa71-267b5194d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1. The K-Nearest Neighbors (KNN) algorithm is a non-parametric machine learning algorithm used\n",
    "for classification and regression tasks. It is a type of instance-based learning, where the algorithm\n",
    "does not explicitly learn a model, but instead, it stores all the training instances and uses them\n",
    "to classify new instances based on their similarity to the training instances.\n",
    "\n",
    "A2. The value of K is typically chosen using cross-validation techniques such as k-fold cross-validation. \n",
    "The optimal value of K depends on the dataset and the problem at hand, but in general, smaller values of\n",
    "K lead to a more flexible model with higher variance and lower bias, while larger values of K lead\n",
    "to a more rigid model with lower variance and higher bias.\n",
    "\n",
    "A3. The KNN classifier is used for classification tasks, where the goal is to predict the class \n",
    "label of a new instance based on the class labels of its k nearest neighbors. The KNN regressor,\n",
    "on the other hand, is used for regression tasks, where the goal is to predict a continuous value\n",
    "for a new instance based on the values of its k nearest neighbors.\n",
    "\n",
    "A4. The performance of KNN can be measured using various metrics such as accuracy, precision, recall,\n",
    "F1 score, mean squared error (MSE), and mean absolute error (MAE).\n",
    "\n",
    "A5. The curse of dimensionality refers to the phenomenon where the performance of KNN deteriorates\n",
    "as the number of features or dimensions increases. This is because as the number of dimensions \n",
    "increases, the distance between any two points becomes more similar, and the concept of proximity\n",
    "becomes less meaningful. One way to address this issue is by using feature selection techniques\n",
    "to reduce the number of dimensions.\n",
    "\n",
    "A6. Missing values can be handled in KNN by either imputing the missing values with a value that\n",
    "is computed based on the values of the k nearest neighbors or by ignoring the instance with missing \n",
    "values during the computation of the distances.\n",
    "\n",
    "A7. The performance of the KNN classifier and regressor depends on the specific problem and dataset \n",
    "at hand. In general, the KNN classifier performs well when the decision boundaries are irregular, \n",
    "while the KNN regressor performs well when the target variable has a continuous range of values.\n",
    "\n",
    "A8. The strengths of the KNN algorithm include its simplicity, interpretability,\n",
    "and ability to handle complex and nonlinear relationships. However, its weaknesses include its\n",
    "sensitivity to the choice of K, the curse of dimensionality, and its computational complexity,\n",
    "especially for large datasets. These weaknesses can be addressed by using appropriate techniques\n",
    "such as cross-validation for choosing K, feature selection for reducing the number of dimensions,\n",
    "and implementing efficient algorithms for computing the distances.\n",
    "\n",
    "A9. Euclidean distance is the distance between two points in a n-dimensional space, computed as\n",
    "the square root of the sum of squared differences between the corresponding coordinates.\n",
    "Manhattan distance, on the other hand, is the distance between two points in a n-dimensional space,\n",
    "computed as the sum of absolute differences between the corresponding coordinates.\n",
    "\n",
    "A10. Feature scaling is important in KNN because the distance between two points is heavily influenced\n",
    "by the scale of the features. Features with larger scales will have a larger impact on the distance,\n",
    "which can result in biased distance computations. Therefore, it is important to scale the features\n",
    "to a similar range before applying the KNN algorithm. Common feature scaling techniques include\n",
    "standardization and normalization.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
