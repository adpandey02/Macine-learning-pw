{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b21590c-c6db-44f6-a547-bb8602a94f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Random Forest Regressor is a type of ensemble learning algorithm that uses a collection \n",
    "of decision trees to make predictions on a continuous variable (i.e., to perform regression).\n",
    "The algorithm works by training a set of decision trees on random subsets of the training data\n",
    "and using the average of their predictions as the final output.\n",
    "\n",
    "Q2. Random Forest Regressor reduces the risk of overfitting by using several techniques such as:\n",
    "Random subspace method: It randomly selects a subset of features for each tree, which reduces \n",
    "the chance of trees relying too much on a single feature.\n",
    "Bootstrap aggregating (Bagging): It uses multiple bootstrap samples of the training data to train \n",
    "different trees. This ensures that each tree has a different subset of the data, which reduces the\n",
    "chances of overfitting.\n",
    "Tree pruning: It removes unnecessary branches of the decision tree, which can lead to overfitting.\n",
    "\n",
    "Q3. Random Forest Regressor aggregates the predictions of multiple decision trees by taking the average\n",
    "of their predictions. Each decision tree is trained on a subset of the data, and the predictions \n",
    "of all the trees are averaged to obtain the final prediction. The aggregation of predictions from \n",
    "multiple trees helps to improve the accuracy and stability of the model.\n",
    "\n",
    "Q4. The hyperparameters of Random Forest Regressor include the number of trees in the forest, \n",
    "the maximum depth of each tree, the minimum number of samples required to split an internal node,\n",
    "the minimum number of samples required to be at a leaf node, and the number of features to\n",
    "consider when looking for the best split.\n",
    "\n",
    "Q5. The main difference between Random Forest Regressor and Decision Tree Regressor is that \n",
    "the former is an ensemble learning method that combines multiple decision trees, while the \n",
    "latter uses a single decision tree. Random Forest Regressor also reduces overfitting by using\n",
    "random subsets of the data and features during the training process, whereas Decision Tree \n",
    "Regressor is prone to overfitting when the tree is too deep.\n",
    "\n",
    "Q6. The advantages of Random Forest Regressor are:\n",
    "Good performance on a wide range of datasets\n",
    "Robust to outliers and noise in the data\n",
    "Automatically selects important features and provides a measure of feature importance\n",
    "The disadvantages are:\n",
    "Computationally expensive to train and predict, especially with large datasets\n",
    "Difficult to interpret the underlying decision-making process\n",
    "Q7. The output of Random Forest Regressor is a continuous variable, which represents the predicted\n",
    "value of the target variable based on the input features.\n",
    "\n",
    "Q8. Yes, Random Forest Regressor can be used for classification tasks by modifying the algorithm\n",
    "to perform classification instead of regression. The main difference is in the output,\n",
    "which is a class label instead of a continuous variable.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
